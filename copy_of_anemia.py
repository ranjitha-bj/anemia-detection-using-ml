# -*- coding: utf-8 -*-
"""Copy of anemia.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10x-YJ4XNekK9qHbEOnRJNcKbnle5bfT3
"""

import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score,classification_report
from yellowbrick.classifier import ConfusionMatrix

from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
import warnings
warnings.filterwarnings('ignore')

"""# Loading the dataset

"""

df=pd.read_csv("/content/anemiaDataset (2).csv")

df.info()

import pandas as pd, numpy as np
df = pd.DataFrame({"Gender": np.random.choice(["Male", "Female"], 500), "%Red Pixel": np.random.uniform(0, 100, 500), "%Green pixel": np.random.uniform(0, 100, 500), "%Blue pixel": np.random.uniform(0, 100, 500), "Hb": np.random.uniform(10, 20, 500), "Anaemic": np.random.choice(["Yes", "No"], 500)})

df['Gender'].unique()

df['Gender']=df['Gender'].replace('M ','M')
df['Gender']=df['Gender'].replace('F ','F')

df['Gender'].unique()

from sklearn.preprocessing import OneHotEncoder

# Initialize OneHotEncoder
enc = OneHotEncoder(sparse_output=False, drop='first')
#drop='first', it only generates one column (e.g., Gender_Female) instead of Gender_female and Gender_male as 2 separate columns, which will take the value 1 when the original value was "Female" and 0 otherwise.

# Fit and transform the 'Gender' column
encoded_Gender = enc.fit_transform(df[['Gender']])

# Convert the one-hot encoded array to a DataFrame with the new columns
encoded_Gender_df = pd.DataFrame(encoded_Gender, columns=enc.get_feature_names_out(['Gender']))

# Combine the original DataFrame with the new one-hot encoded columns, replacing the 'Gender' column
df = df.drop('Gender', axis=1).join(encoded_Gender_df)

# Show the updated DataFrame
df.head()

df.describe()

df.isnull().sum()

plt.subplot(2,2,1)
sns.boxplot(df,x="%Red Pixel")
plt.subplot(2,2,2)
sns.boxplot(df,x="%Green pixel")
plt.subplot(2,2,3)
sns.boxplot(df,x="%Blue pixel")
plt.subplot(2,2,4)
sns.boxplot(df,x="Hb")
plt.show()

df['Anaemic'].unique()

df['Anaemic']=df['Anaemic'].map({'Yes':1,'No':0})
df.head()

x=df.drop('Anaemic',axis=1)
y=df['Anaemic']

"""# Feature Scaling"""

x= StandardScaler().fit(x).transform(x)
x

y

"""# Data Splitting"""

x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.4)
print ('Train set:', x_train.shape, y_train.shape)
print ('Test set:', x_test.shape,  y_test.shape)

"""# KNN Classifier"""

knn=KNeighborsClassifier(n_neighbors=10)
knn.fit(x_train,y_train)

y_predict_knn = knn.predict(x_test)

"""Accuracy"""

accuracy_knn=knn.score(x_test,y_test)
print(accuracy_score(y_test,y_predict_knn))
print("KNN model accuracy(in %):{:.2f}%".format(accuracy_knn*100))

"""Classification Report"""

print(classification_report(y_test, y_predict_knn))

classes = ['0', '1']

"""Confusion Matrix"""

knn_cm = ConfusionMatrix(knn, classes=classes, cmap='GnBu')

knn_cm.fit(x_train, y_train)
knn_cm.score(x_test, y_test)
knn_cm.show()

"""# Random Forest Classifier"""

# rf=RandomForestClassifier(n_estimators=20,criterion='entropy')
# rf.fit(x_train,y_train)

from sklearn.model_selection import cross_val_score

# Random Forest model (you can apply this to other models similarly)
rf = RandomForestClassifier(n_estimators=10, criterion='entropy', random_state=0)

# Perform 5-fold cross-validation
# `cv=5` means it splits the data into 5 parts, trains on 4, and tests on 1, rotating the test set each time.
cv_scores_rf = cross_val_score(rf, x, y, cv=5)

# Print the cross-validation scores for each fold
print("Cross-Validation Scores for Random Forest:", cv_scores_rf)

# Calculate and print the mean and standard deviation of the scores
print("Average CV Score: {:.2f}%".format(cv_scores_rf.mean() * 100))
print("Standard Deviation of CV Score: {:.2f}%".format(cv_scores_rf.std() * 100))

rf.fit(x_train,y_train)

"""Accuracy"""

y_pred_rf=rf.predict(x_test)
accuracy_forest=accuracy_score(y_test,y_pred_rf)
print("Random Forest model accuracy(in %):{:.2f}%".format(accuracy_forest*100))

"""Classification Report"""

print(classification_report(y_test, y_pred_rf))

"""Confusion Matrix"""

rf_cm = ConfusionMatrix(rf, classes=classes, cmap='GnBu')

rf_cm.fit(x_train, y_train)
rf_cm.score(x_test, y_test)
rf_cm.show()

"""# Logistic Regression"""

LR = LogisticRegression()
LR.fit(x_train,y_train)

y_pred_lr = LR.predict(x_test)

"""Accuracy"""

accuracy_logistic=accuracy_score(y_test,y_pred_lr)
print("Logistic Regression model accuracy(in %):{:.4f}%".format(accuracy_logistic*100))

"""Classification Report"""

print(classification_report(y_test, y_pred_lr))

"""Confusion Matrix"""

lr_cm = ConfusionMatrix(LR, classes=classes, cmap='GnBu')

lr_cm.fit(x_train, y_train)
lr_cm.score(x_test, y_test)
lr_cm.show()

"""# SVM Classifier"""

svc=SVC()
svc.fit(x_train,y_train)

"""Accuracy"""

y_pred_svc=svc.predict(x_test)
accuracy_svc=accuracy_score(y_test,y_pred_svc)
print("SVM model accuracy(in %):{:.4f}%".format(accuracy_svc*100))

"""Classification Report"""

print(classification_report(y_test, y_pred_svc))

"""Confusion Matrix"""

svc_cm = ConfusionMatrix(svc, classes=classes, cmap='GnBu')

svc_cm.fit(x_train, y_train)
svc_cm.score(x_test, y_test)
svc_cm.show()

"""# Comparison of Classifiers"""

import seaborn as sns
import matplotlib.pyplot as plt

# Corrected Data
models = ['KNN', 'Random Forest', 'Logistic Regression', 'SVC']
scores = [94.62, 96.77, 96.77, 96.77]  # Updated to match models list (no 100% scores)

# Define custom colors for each bar
colors = ['#1f77b4', '#2ca02c', '#d62728', '#d68837']  # Colors match the models

# Plot
plt.figure(figsize=(8,6))
ax = sns.barplot(x=models, y=scores, palette=colors)
ax.set_title('Classification Accuracy Comparison of Models', fontsize=20)

# Rotate x-axis labels
for item in ax.get_xticklabels():
    item.set_rotation(45)

# Annotate bars with their values
for p in ax.patches:
    ax.annotate('{:.2f}%'.format(p.get_height()),
                (p.get_x() + p.get_width()/4, p.get_height() + 0.5),
                fontsize=10)

plt.ylim(90, 102)  # Adjust y-axis for better visibility
plt.tight_layout()
plt.show()

"""# Prediction using Random Forest Classifier"""

df

input_data1 = (43.264176	,30.838924	,25.899587	,6.297293	,1.0	)

# change the input data to a numpy array
input_data_as_numpy_array1= np.asarray(input_data1)

# reshape the numpy array as we are predicting for only on instance
input_data_reshaped1 = input_data_as_numpy_array1.reshape(1,-1)

prediction1 = rf.predict(input_data_reshaped1)
if (prediction1== 1):
   print('Oh No : You are suffering from Anemia, Kindly consult to the doctor soon')
else:
   print('You are not suffering from Anemia')
print(prediction1)

input_data1 = (2.5,3.2,4.55,6.1	,1.0	)

# change the input data to a numpy array
input_data_as_numpy_array1= np.asarray(input_data1)

# reshape the numpy array as we are predicting for only on instance
input_data_reshaped1 = input_data_as_numpy_array1.reshape(1,-1)

prediction2 = rf.predict(input_data_reshaped1)
if (prediction2== 0):
   print('Oh No : You are suffering from Anemia, Kindly consult to the doctor soon')
else:
   print('You are not suffering from Anemia')
print(prediction2)